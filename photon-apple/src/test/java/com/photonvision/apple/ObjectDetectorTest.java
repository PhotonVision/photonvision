// ===----------------------------------------------------------------------===//
//
// This source file is part of the Swift.org open source project
//
// Copyright (c) 2024 Apple Inc. and the Swift.org project authors
// Licensed under Apache License v2.0
//
// See LICENSE.txt for license information
// See CONTRIBUTORS.txt for the list of Swift.org project authors
//
// SPDX-License-Identifier: Apache-2.0
//
// ===----------------------------------------------------------------------===//

package com.photonvision.apple;

import static org.junit.jupiter.api.Assertions.*;

import java.lang.foreign.MemorySegment;
import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.condition.EnabledOnOs;
import org.junit.jupiter.api.condition.OS;
import org.opencv.core.CvType;
import org.opencv.core.Mat;
import org.swift.swiftkit.ffm.AllocatingSwiftArena;

/**
 * Tests for ObjectDetector functionality
 *
 * <p>Note: These tests require a valid CoreML model file to run successfully. On non-macOS
 * platforms, the ObjectDetector will fail to initialize.
 */
class ObjectDetectorTest {

    @BeforeAll
    static void setup() {
        CoreMLTestUtils.initializeLibraries();
    }

    @Test
    void test_ImageUtils_getPixelFormatFromChannels() {
        // Test BGR (3 channels)
        assertEquals(0, ImageUtils.getPixelFormatFromChannels(3)); // BGR

        // Test BGRA (4 channels)
        assertEquals(2, ImageUtils.getPixelFormatFromChannels(4)); // BGRA

        // Test Grayscale (1 channel)
        assertEquals(4, ImageUtils.getPixelFormatFromChannels(1)); // GRAY
    }

    @Test
    void test_ImageUtils_pixelFormatToString() {
        assertEquals("BGR", ImageUtils.pixelFormatToString(0));
        assertEquals("RGB", ImageUtils.pixelFormatToString(1));
        assertEquals("BGRA", ImageUtils.pixelFormatToString(2));
        assertEquals("RGBA", ImageUtils.pixelFormatToString(3));
        assertEquals("GRAY", ImageUtils.pixelFormatToString(4));
        assertEquals("UNKNOWN", ImageUtils.pixelFormatToString(99));
    }

    @Test
    void test_ObjectDetector_detectFake_returnsSyntheticResults() {
        // Test that we can successfully receive DetectionResult data from Swift
        // This validates the Swiftâ†’Java data passing without requiring a CoreML model

        try (var arena = AllocatingSwiftArena.ofConfined()) {
            // Create detector (doesn't need a real model for detectFake)
            ObjectDetector detector = ObjectDetector.init("/fake/path/model.mlmodel", arena);
            assertNotNull(detector);

            // Create a dummy BGRA image
            try (var frameArena = AllocatingSwiftArena.ofConfined()) {
                Mat testImage = new Mat(480, 640, CvType.CV_8UC4); // 480 rows x 640 cols = BGRA
                int totalBytes = 480 * 640 * 4;
                MemorySegment imageData = frameArena.allocate(totalBytes, 1);

                // Call detectFake which returns synthetic test data
                DetectionResultArray results =
                        detector.detectFake(
                                imageData,
                                640L,
                                480L,
                                2, // BGRA format
                                0.5,
                                0.4,
                                frameArena);

                // Validate we got the expected synthetic results
                assertNotNull(results);
                assertEquals(3, results.count(), "Should return 3 fake detection results");

                // Validate first detection result
                DetectionResult det0 = results.get(0, frameArena);
                assertNotNull(det0);

                assertEquals(0.1, det0.getX(), 0.001, "First detection X coordinate");
                assertEquals(0.2, det0.getY(), 0.001, "First detection Y coordinate");
                assertEquals(0.3, det0.getWidth(), 0.001, "First detection width");
                assertEquals(0.4, det0.getHeight(), 0.001, "First detection height");
                assertEquals(1, det0.getClassId(), "First detection class ID");
                assertEquals(0.95, det0.getConfidence(), 0.001, "First detection confidence");

                // Validate second detection result
                DetectionResult det1 = results.get(1, frameArena);
                assertNotNull(det1);
                assertEquals(0.5, det1.getX(), 0.001);
                assertEquals(0.5, det1.getY(), 0.001);
                assertEquals(0.2, det1.getWidth(), 0.001);
                assertEquals(0.2, det1.getHeight(), 0.001);
                assertEquals(2, det1.getClassId());
                assertEquals(0.87, det1.getConfidence(), 0.001);

                // Validate third detection result
                DetectionResult det2 = results.get(2, frameArena);
                assertNotNull(det2);
                assertEquals(0.7, det2.getX(), 0.001);
                assertEquals(0.1, det2.getY(), 0.001);
                assertEquals(0.15, det2.getWidth(), 0.001);
                assertEquals(0.25, det2.getHeight(), 0.001);
                assertEquals(3, det2.getClassId());
                assertEquals(0.72, det2.getConfidence(), 0.001);

                // Test conversion to pixel coordinates
                int pixelX = (int) (det0.getX() * 640);
                int pixelY = (int) (det0.getY() * 480);
                assertEquals(64, pixelX, "Pixel X coordinate (0.1 * 640)");
                assertEquals(96, pixelY, "Pixel Y coordinate (0.2 * 480)");

                testImage.release();
            }
        }
    }

    @Test
    void test_DetectionResultArray_empty() {
        // Test that empty DetectionResultArray works
        // Note: Can't easily create an empty DetectionResultArray from Java
        // since it requires Swift side initialization. This test is placeholder.

        // In real usage, an empty array would be returned from Swift detect() method
        // when no objects are detected
        assertTrue(true, "Empty array handling tested via integration tests");
    }

    @Test
    @EnabledOnOs(OS.MAC)
    void test_ObjectDetector_detect_withSyntheticImage() {
        // This test requires a real CoreML model

        String modelPath = System.getenv("TEST_COREML_MODEL_PATH");
        if (modelPath == null || modelPath.isEmpty()) {
            System.out.println("Skipping test: TEST_COREML_MODEL_PATH not set");
            return;
        }

        try (var arena = AllocatingSwiftArena.ofConfined()) {
            ObjectDetector detector = ObjectDetector.init(modelPath, arena);

            // Create a synthetic BGRA test image
            try (var frameArena = AllocatingSwiftArena.ofConfined()) {
                Mat testImage = new Mat(480, 640, CvType.CV_8UC4);
                testImage.setTo(new org.opencv.core.Scalar(128, 128, 128, 255)); // Gray BGRA

                int totalBytes = 480 * 640 * 4;
                MemorySegment imageData = frameArena.allocate(totalBytes, 1);
                byte[] buffer = new byte[totalBytes];
                testImage.get(0, 0, buffer);
                imageData.copyFrom(MemorySegment.ofArray(buffer));

                DetectionResultArray results =
                        detector.detect(imageData, 640L, 480L, 2, 0.5, 0.4, frameArena);

                // Just verify we get a result array (may be empty for synthetic image)
                assertNotNull(results);
                assertTrue(results.count() >= 0);

                testImage.release();
            }
        }
    }

    @Test
    void test_DetectionResult_accessors() {
        // Test that DetectionResult accessors work
        // Can only create DetectionResult via Swift, so this is tested
        // in integration tests
        assertTrue(true, "DetectionResult accessors tested via integration tests");
    }

    @Test
    void test_ObjectDetector_reusablePattern() {
        // Test the PhotonVision-style reusable detector pattern
        String modelPath = System.getenv("TEST_COREML_MODEL_PATH");
        if (modelPath == null || modelPath.isEmpty()) {
            System.out.println("Skipping test: TEST_COREML_MODEL_PATH not set");
            return;
        }

        try (var detectorArena = AllocatingSwiftArena.ofConfined()) {
            ObjectDetector detector = ObjectDetector.init(modelPath, detectorArena);

            // Simulate processing multiple frames
            for (int i = 0; i < 3; i++) {
                try (var frameArena = AllocatingSwiftArena.ofConfined()) {
                    Mat image = new Mat(480, 640, CvType.CV_8UC4);
                    image.setTo(new org.opencv.core.Scalar(128, 128, 128, 255));

                    int totalBytes = 480 * 640 * 4;
                    MemorySegment imageData = frameArena.allocate(totalBytes, 1);
                    byte[] buffer = new byte[totalBytes];
                    image.get(0, 0, buffer);
                    imageData.copyFrom(MemorySegment.ofArray(buffer));

                    DetectionResultArray results =
                            detector.detect(imageData, 640L, 480L, 2, 0.5, 0.4, frameArena);

                    assertNotNull(results);
                    image.release();
                }
            }
        } catch (Exception e) {
            // Expected if model path not set or on non-macOS
            System.out.println("Test skipped: " + e.getMessage());
        }
    }
}
